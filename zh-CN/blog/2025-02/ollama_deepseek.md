---
lastUpdated: true
commentabled: true
recommended: true
title: 用 Ollama 和 Chatbox 在本地运行 DeepSeek 模型，打造你的 AI 小秘书
description: 用 Ollama 和 Chatbox 在本地运行 DeepSeek 模型，打造你的 AI 小秘书
date: 2025-02-10 11:00:00
pageClass: blog-page-class
---

# 用 Ollama 和 Chatbox 在本地运行 DeepSeek 模型，打造你的 AI 小秘书 #

## DeepSeek R1 简单指南：架构、训练、本地部署和硬件要求 ##

### DeepSeek推出的LLM推理新策略 ###

DeepSeek最近发表的论文DeepSeek-R1中介绍了一种创新的方法，通过强化学习（RL）提升大型语言模型（LLM）的推理能力。这项研究在如何仅依靠强化学习而不是过分依赖监督式微调的情况下，增强LLM解决复杂问题的能力上，取得了重要进展。

### DeepSeek-R1 技术概述 ###

#### 模型架构 ####

DeepSeek-R1 不是一个单独的模型，而是包括DeepSeek-R1-Zero和DeepSeek-R1的模型系列。

下面我来阐明 DeepSeek-R1 和 DeepSeek-R1-Zero 之间的关键差异：

#### 主要区别 ####

**DeepSeek-R1-Zero**是团队初步尝试仅用纯强化学习而不进行任何监督式微调的实验。他们从基础模型出发，直接运用强化学习，让模型通过不断试错来发展其推理能力。这种方法虽然取得了较好的成果（在 AIME 2024 测试中达到了 71% 的准确率），但在可读性和语言连贯性上存在明显不足。该模型拥有 6710 亿个参数，使用了混合专家（MoE）架构，其中每个词触发的参数约为 370 亿。此模型展现了一些新兴的推理行为，例如自我核查、反思和长链推理（CoT）。

与之对比，**DeepSeek-R1**采用了更复杂的多阶段训练方法。它不仅仅采用强化学习，而是先在一小组精心挑选的示例（称为“冷启动数据”）上进行监督式微调，然后再应用强化学习。这种方法克服了 DeepSeek-R1-Zero 的局限，同时取得了更优的表现。这个模型同样维持了 6710 亿的参数数量，但在回答的可读性和条理性上有所提高。

#### 训练过程对比 ####

*训练方法概述*

- 强化学习：不同于传统依赖监督学习的模型，DeepSeek-R1 大规模采用了强化学习。此训练方法利用群体相对策略优化（GRPO），重点提升精度和格式化奖励，以增强推理能力，无需依赖大量标注数据。
- 蒸馏技术：为普及高效能模型，DeepSeek 也推出了 R1 的蒸馏版本，参数规模从15亿到700亿不等。这些模型采用了如Qwen和Llama等架构，表明即使是较小和更高效的模型也能包含复杂的推理能力。蒸馏过程通过使用 DeepSeek-R1 生成的合成推理数据对这些小型模型进行微调，以较低的计算成本保持高性能。

*DeepSeek-R1-Zero 的训练流程*

- 起始于基础模型
- 直接应用强化学习
- 根据准确度和格式采用简单奖励机制

*DeepSeek-R1 的训练流程包含四个阶段*

- 初始阶段采用数千个高品质样本进行监督式微调
- 针对推理任务进行强化学习
- 通过拒绝抽样方法收集新的训练数据
- 对所有类型的任务进行最终强化学习

*性能指标*

- 推理基准测试：DeepSeek-R1 在各种基准测试中表现出色：
  - AIME 2024：实现了 79.8% 的通过率，高于 OpenAI 的 o1–1217 的 79.2%。
  - MATH-500：得分高达 97.3%，略优于 o1–1217 的 96.4%。
  - SWE-bench 验证：在编程任务中表现优越，证明了其编程能力。
- 成本效率：DeepSeek-R1 的 API 服务每百万输入令牌的成本为0.14美元，比 OpenAI 的类似模型便宜很多。

### 局限性及未来发展 ###

该论文指出了若干改进领域：

- 模型在处理需要特定输出格式的任务时偶尔会遇到困难。
- 软件工程相关任务的性能还有提升空间。
- 在多语言环境下，语言混合带来了挑战。
- 少样本提示通常会导致性能下降。

未来的研究将致力于解决这些问题，并拓展模型在函数调用、多轮交互和复杂角色扮演场景等领域的能力。

### 部署与可获取性 ###

#### 开源与许可 ####

DeepSeek-R1及其变体基于 MIT 许可证发布，支持开源合作和商业使用，包括模型蒸馏。此举对促进创新和降低人工智能模型开发门槛具有关键意义。

#### 模型格式 ####

这些模型及其蒸馏版本支持 GGML、GGUF、GPTQ 和 HF 等多种格式，使其在本地部署上具有灵活性。

### DeepSeek使用方式 ###

我们可以通过三种方式使用DeepSeek：官方web访问、API使用、本地部署。

#### 通过DeepSeek聊天平台进行网页访问 ####

DeepSeek聊天平台提供了一个友好的用户界面，允许用户无需任何设置即可与DeepSeek-R1进行互动。

- 访问步骤：
  - 浏览至DeepSeek聊天平台
  - 注册一个账号，或者如果您已有账号，直接登录。
  - 登录后，可以选择“深度思考”模式，体验DeepSeek-R1的逐步推理功能。

#### 通过 DeepSeek API 访问 ####

DeepSeek 提供了一个与 OpenAI 格式兼容的 API，方便开发者将其嵌入各种应用程序中进行程序化访问。

当前注册还可以享有10块钱的赠送额度

**使用 API 的步骤**

- 获取 API 密钥：
  - 访问DeepSeekAPI平台，注册账号并生成您的专属 API 密钥。
- 配置您的环境：
  - 设置base_url为https://api.deepseek.com/v1。
  - 使用您的 API 密钥进行认证，通常在 HTTP 头部通过 Bearer Token 进行。
- 发起 API 调用：
  - 利用 API 向 DeepSeek-R1 发送指令并接收响应。
  - 您可以在DeepSeekAPI文档中找到详细的文档和示例。

```python
# 请先安装 OpenAI SDK：`pip3 install openai`

from openai import OpenAI

client = OpenAI(api_key="<DeepSeek API Key>", base_url="https://api.deepseek.com")

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"},
    ],
    stream=False
)

print(response.choices[0].message.content)
```

#### 在本地部署 DeepSeek-R1 ####

**两种模型（R1 和 R1-Zero）**

- 硬件需求：由于模型规模庞大，完整模型需要较强的硬件支持。推荐使用具有大量视频内存（VRAM）的 GPU，例如 Nvidia RTX 3090 或更高级别。如果使用 CPU，你至少需要 48GB 的 RAM 和 250GB 的磁盘空间，但若不使用 GPU 加速，性能会显著下降。
- 蒸馏模型：对于硬件要求不那么高的本地部署，DeepSeek 提供了参数范围从 15 亿到 700 亿的蒸馏版本，适合硬件配置较低的系统。例如，一个 7B 参数的模型可以在至少拥有 6GB VRAM 的 GPU 上运行，或在大约 4GB RAM 的 CPU 上运行 GGML/GGUF 格式。

**DeepSeek本地部署步骤**

1. Ollama:

您可以使用Ollama来在本地部署和运行模型：（Ollama 是一个工具，允许您在个人计算机上本地运行开源 AI 模型。您可以从这里下载它：ollama.com/download）

2. 验证 Ollama 安装情况

在进一步操作之前，我们需要确保 Ollama 已经正确安装。请打开您的终端或命令提示符，输入以下命令：

```bash
ollama--version
```

如果您看到版本号显示出来，说明安装成功了！如果没有显示，那么请仔细检查您是否已按照安装步骤正确执行。

3. Download Deepseek R1

通过 Ollama 下载 DeepSeek R1 非常便捷。您只需在终端中执行以下命令：

```bash
ollama run deepseek-r1
```

默认版本是：7B 模型（CPU 大概需要4G内存，1.5B大概需要1.7G内存），DeepSeek R1 的参数范围从 1.5B 到 671B，可以使用ollama run deepseek-r1:1.5b适合自己的模型版本，下载时间可能会根据您的网络速度而异。下载期间，可以喝杯水或者活动一下身体，稍作等待吧？

4. 运行 DeepSeek R1

下载完成后，您就可以启动 DeepSeek R1 了。使用以下命令：

```bash
ollama run deepseek-r1
```

就这样，您已经在本地机器上顺利启动了 DeepSeek R1！感觉如何，是不是so easy？

**本地运行试用Deepseek R1**

现在我们已经成功启动了 DeepSeek R1，接下来就是更加有趣的部分——实际使用它！让我们一起探索这款强大的 AI 模型能做些什么神奇的事情。

1. 创意写作
2. 
DeepSeek R1 在创意写作方面非常擅长。尝试给它这样一个写作提示：

```text
写一个短故事，讲述一个机器人发现自己拥有情感的经历。
```

您会对它输出的既有创意又条理清晰的故事感到惊喜！

### 本地运行 DeepSeek R1：探索人工智能的未来可能 ###

这次我们讲解了在本地环境中使用 Ollama 运行 DeepSeek R1。 DeepSeek R1 这样的强大 AI 模型运行在个人电脑上，不仅仅是展示一项引人注目的技术，更是我们对未来人工智能发展方向的一种探索。这样做让高级语言模型的强大功能直接落在我们的手中，使得个性化定制、深入实验以及更好的隐私保护成为可能。让我们想象以下几种可能：

- 开发者可以为特定领域创建出高度定制化的 AI 助手。
- 研究人员在没有云服务依赖的情况下，可以更自由地试验 AI 模型。
- 对于那些注重隐私的用户，他们可以利用先进的 AI 技术，而无需担心个人数据泄露。


## Ollama 和 Chatbox 在本地运行 DeepSeek 模型 ##

哈喽，各位未来 程序猿/媛们！👋 我是你们的AI探索小助手，今天给大家分享一个超酷的玩法：告别动不动就联网的AI，直接在咱们自己的电脑上养一个AI小秘书！听起来是不是有点科幻？其实一点也不难，只需要两个神器——`Ollama`和`Chatbox`，就能搞定！

想象一下，以后写代码遇到bug，直接问本地的AI，不用担心隐私泄露，也不用担心网络不稳定，是不是很爽？😎

### 准备工作：磨刀不误砍柴工 ###

在开始之前，我们需要准备好“砍柴”的工具：

- 你的电脑：一台装有 Windows, macOS, 或者 Linux 的电脑，最好是配置稍微好一点的，毕竟跑大模型还是需要点硬件支持的。
- Ollama: 这个是咱们本地运行大模型的“发动机”，负责把模型跑起来。
- Chatbox: 这是一个用户友好的界面，让我们可以像聊天一样和本地大模型互动，告别黑漆漆的命令行。

### 第一步：给电脑装上“发动机”——Ollama ###

首先，我们得把 Ollama 下载到电脑上。打开浏览器，访问 Ollama官网，你会看到一个简洁的页面，上面有 macOS、Linux 和 Windows 三个系统的下载选项。

**注意事项**

- Ollama 目前对 Windows 的支持还比较新，所以确保你的 Windows 系统是 Windows 10 或更高版本。
- 安装过程很简单，基本就是一路“下一步”，按照提示操作就行。

安装完成后，打开命令行终端（Windows 上是 CMD 或者 PowerShell，macOS 和 Linux 上是 Terminal），输入`ollama --version`，如果能看到 Ollama 的版本号，就说明安装成功了！🎉

>小知识点： 命令行终端是啥？  简单来说，它就像一个可以和电脑“对话”的窗口，你可以输入一些命令，让电脑帮你做一些事情。

### 第二步：挑选你的AI“宠物”——下载模型 ###

Ollama 本身不带任何 AI 模型，我们需要自己下载。你可以把它想象成一个“模型仓库”，里面有各种各样的 AI 模型，你可以根据自己的需求选择下载。

我们以 *deepseek-r1* 这个模型为例（这个模型在图片里出现了哦！）。它是一个推理能力不错的模型，而且体积适中，适合在本地运行。

在命令行终端输入以下命令：

```bash
ollama run deepseek-r1:7b
```

**命令解析**

- `ollama run`：这是 Ollama 的命令，表示运行一个模型。
- `deepseek-r1`：这是模型的名字。
- `:7b`：这表示模型的版本，这里选择的是 70亿参数的版本。模型越大，性能通常越好，但对硬件的要求也越高。

**温馨提示**

- 第一次运行这个命令时，Ollama 会自动下载模型文件，这个过程可能需要一段时间，取决于你的网速。
- 下载完成后，Ollama 会启动模型，并进入一个交互式的聊天界面。你可以直接在终端里和模型对话！
- 如果你想了解更多关于 deepseek-r1 的信息，可以在网上搜索它的名字，或者查看模型的 GitHub 仓库。

### 第三步：给AI小秘书穿上“漂亮衣服”——安装并配置 Chatbox ###

虽然可以直接在命令行和 Ollama 互动，但那种黑漆漆的界面实在是不友好。所以，我们需要给我们的 AI 小秘书穿上一件“漂亮衣服”—— *Chatbox*！

打开浏览器，访问 Chatbox官网，你会看到一个醒目的下载按钮。根据你的操作系统选择对应的版本下载并安装。

**Chatbox 的配置**

- 打开 Chatbox，你会看到一个设置界面。
- 在“模型提供方”下拉菜单中，选择 “Ollama API”。
- 在“API 域名”中，填入 `http://127.0.0.1:11434`。这个地址是 Ollama 默认的 API 地址。
- 在“模型”下拉菜单中，选择你刚刚下载的模型，也就是 deepseek-r1:7b。
- 根据你的喜好，调整“上下文的消息数量上限”和“严肃与想象(Temperature)”这两个参数。
  - *上下文的消息数量上限*：这个参数决定了 Chatbox 记住多少轮对话。设置得越大，AI 就越能理解你的上下文，但也会占用更多的内存。
  - *严肃与想象(Temperature)*：这个参数控制 AI 的“创造力”。值越高，AI 就越倾向于生成随机和新颖的回答；值越低，AI 就越倾向于生成保守和符合逻辑的回答。
- 点击“保存”按钮。

**配置完成！**🎉 你现在可以开始和你的本地 AI 小秘书聊天啦！在 Chatbox 的聊天框里输入你的问题，它会快速地给出答案。

### 第四步：尽情调戏你的AI小秘书！ ###

现在，你可以尽情地调戏你的AI小秘书了！你可以问它各种各样的问题，比如：

- “帮我写一段 Python 代码，实现快速排序算法。”
- “解释一下什么是区块链技术。”
- “给我讲一个笑话。”
- “用五句话概括一下《红楼梦》的故事。”

你会发现，你的AI小秘书真的很聪明，它可以帮你解决各种各样的问题，而且速度很快，不用担心网络延迟！

### 总结：本地AI，未来可期！ ###

通过 Ollama 和 Chatbox，我们成功地在本地搭建了一个强大的 AI 小秘书。这不仅可以保护我们的隐私，还可以让我们更自由地探索 AI 的可能性。
